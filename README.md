# Agent-X

Deep dive into Generative AI with the 5 day Google workshop conducted on Kaggle.

- Prompt Engineering
- Embeddings and Similarity
- Retrieval Agumented Generation (RAG)
- Agents
- LangGraph
- Fine-Tuning
- Google Gemini API

## Prompt Engineering 
- Temperature, Top-K and Top-P Parameters
- Zero Shot
- One Shot
- Few Shot
- Chain of Thought
- Reason and Act (ReAct)

## Embeddings and Similarity

## Retrieval Agumented Generation (RAG)

## Agents

## LangGraph

## Fine-Tuning

## Google Gemini API

# Evolution of LLM Models
GPT 1
	- Unsupervised PreTraining
	- Unlabelled Text

GPT 2 
	- 10 times larger dataset for training, 10 times the parameters, larger brain, whole universe of text
	- generate super realistic texts, coherent, accuracy

GPT 3
	- 175 billlion parameters
	- Few shot learning
	- no need for mountains of labelled data

GPT 3.5 and 4
	- Doubled down on dialogue, multi modal 
	- Handle images and text as well - Multi Modal
	- longer context window
	- remember longer

LamDA - Language Model for Dialogue Applications
	- smooth talker
	- natural sounding , human sounding, less robotic

Gopher
	- refining the training process
	- data quality and optimization

PalM Pathways Language Model
	- 540 billion parameters
	- Common sense reasoning
	- Arithmetic Reasoning
	
PalM-2
	- code generation
	- works with fewer parameters
	- Smarter, better reasoning, code generation

Gemini
	- multi modal 
	- text, video, audio
	- Swiss Army knife
	- millions of tokens

Gemini Nano
	- AI to smaller devices like Smartphones and Wearables

Meta
	- Llama

Mistral AI
	- MistralLM

xAI
	- Grok1

## Fine Tuning Techniques
	- Supervised finetuneing SFT
		- become expert in a specific area
		- give a crash course
	- RLHF - reinforcement learning with human feedback
		- Reward Model
			- good vs bad model â€” talent show
	- PEFT Parameter Efficient Fine Tuning
			- retrain specific parts
			- Adapters 

## Prompt Engineering
	- Well crafted prompt
		- more relevant, creative
	- What you ask, how you ask it
	- Zero Shot Prompting
		- Task description based on internal knowledge
	- Give few examples
		- Guide
	- Chain of thought prompting
		- Sequential Prompting

## Sampling Techniques

## Inference Optimization Techniques
		- Quantization
			- Lower Precision
		- Distillation	
			- Train a smaller, faster student model
		- Output preserving methods
			- Flash Attention
		- Prefix Caching
			- Chatbots
			- Long Documents
			- Cache Previous outputs - remember previous answers
		- Speculative Decoding
			- Team of Assistants working in parallel

